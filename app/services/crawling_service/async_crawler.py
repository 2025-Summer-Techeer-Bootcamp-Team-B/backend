import asyncio
import aiohttp
import time
from datetime import datetime
from typing import Dict, List
from app.services.article_service.save import save_articles_batch
from app.services.crawling_service.rss_fetcher import fetch_rss_feed_async
from app.core.database import SessionLocal
from app.services.crawling_service.article_processor import process_article_with_summary

RSS_FEEDS = {
    # "í•œêµ­ê²½ì œ": {
    #     "ì¦ê¶Œ": "https://www.hankyung.com/feed/finance",
    #     "ê²½ì œ": "https://www.hankyung.com/feed/economy",
    #     "ë¶€ë™ì‚°": "https://www.hankyung.com/feed/realestate",
    #     "IT": "https://www.hankyung.com/feed/it",
    #     "ì •ì¹˜": "https://www.hankyung.com/feed/politics",
    #     "êµ­ì œ": "https://www.hankyung.com/feed/international",
    #     "ì‚¬íšŒ": "https://www.hankyung.com/feed/society",
    #     "ë¬¸í™”": "https://www.hankyung.com/feed/life",
    #     "ìŠ¤í¬ì¸ ": "https://www.hankyung.com/feed/sports",
    #     "ì—°ì˜ˆ": "https://www.hankyung.com/feed/entertainment"
    # },
    "SBSë‰´ìŠ¤": {
        "ì •ì¹˜": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER",
        "ê²½ì œ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02&plink=RSSREADER",
        "ì‚¬íšŒ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=03&plink=RSSREADER",
        "êµ­ì œ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=07&plink=RSSREADER",
        "ë¬¸í™”": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=08&plink=RSSREADER",
        "ì—°ì˜ˆ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=14&plink=RSSREADER",
        "ìŠ¤í¬ì¸ ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=09&plink=RSSREADER"
    },
    # "ë§¤ì¼ê²½ì œ":{
    #     "ê²½ì œ":"https://www.mk.co.kr/rss/30100041",
    #     "ì •ì¹˜":"https://www.mk.co.kr/rss/30200030",
    #     "ì‚¬íšŒ":"https://www.mk.co.kr/rss/50400012",
    #     "êµ­ì œ":"https://www.mk.co.kr/rss/30300018",
    #     "ì¦ê¶Œ":"https://www.mk.co.kr/rss/50200011",
    #     "ë¶€ë™ì‚°":"https://www.mk.co.kr/rss/50300009",
    #     "ë¬¸í™”":"https://www.mk.co.kr/rss/30000023",
    #     "ìŠ¤í¬ì¸ ":"https://www.mk.co.kr/rss/71000001",
    #     "IT":"https://www.mk.co.kr/rss/50700001"
    # },
}

def print_section(title: str):
    print("=" * 80)
    print(title)
    print("=" * 80)

async def scrape_category_async(
    session: aiohttp.ClientSession,
    category: str,
    rss_url: str,
    press: str,
    semaphore: asyncio.Semaphore
) -> List[Dict]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ ë¹„ë™ê¸° í¬ë¡¤ë§ (ë™ì‹œì„± ì œí•œ)
    """
    print(f"ğŸ“° {press} - {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
    start_time = time.time()
    article_urls = await fetch_rss_feed_async(session, rss_url)
    if not article_urls:
        print(f"   âŒ {category} ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    print(f"   ğŸ“Š {category} ì¹´í…Œê³ ë¦¬: {len(article_urls)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    async def process_with_semaphore(url, index):
        async with semaphore:
            await asyncio.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            return await process_article_with_summary(session, url, category, press, index, len(article_urls))
    tasks = [process_with_semaphore(url, i + 1) for i, url in enumerate(article_urls)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    successful_articles = [r for r in results if r and not isinstance(r, Exception) and isinstance(r, dict)]
    elapsed = time.time() - start_time
    print(f"   ğŸ¯ {category} ì™„ë£Œ: {len(successful_articles)}ê°œ ì„±ê³µ (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
    print("   " + "-" * 70)
    return successful_articles

async def scrape_all_articles_async(max_concurrent: int = 10, save_to_db: bool = True):
    """
    ì „ì²´ ì–¸ë¡ ì‚¬/ì¹´í…Œê³ ë¦¬ ë¹„ë™ê¸° í¬ë¡¤ë§ ë° DB ì €ì¥
    """
    scraped_articles = []
    start_time = time.time()
    processed_articles = 0
    failed_articles = 0
    print_section(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âš¡ ë™ì‹œ ì²˜ë¦¬ ìˆ˜: {max_concurrent}")
    semaphore = asyncio.Semaphore(min(max_concurrent, 10))
    connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
    timeout = aiohttp.ClientTimeout(total=60)
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        category_tasks = [
            scrape_category_async(session, category, rss_url, press, semaphore)
            for press, categories in RSS_FEEDS.items()
            for category, rss_url in categories.items()
        ]
        category_results = await asyncio.gather(*category_tasks, return_exceptions=True)
        for result in category_results:
            if isinstance(result, Exception):
                print(f"âŒ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                failed_articles += 1
            elif isinstance(result, list):
                scraped_articles.extend(result)
                processed_articles += len(result)
    total_time = time.time() - start_time
    success_rate = (processed_articles / (processed_articles + failed_articles) * 100) if (processed_articles + failed_articles) > 0 else 0
    print_section(f"ğŸ‰ ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ì „ì²´ í†µê³„:")
    print(f"   â€¢ ì„±ê³µ: {processed_articles}ê°œ")
    print(f"   â€¢ ì‹¤íŒ¨: {failed_articles}ê°œ")
    print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
    print(f"   â€¢ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
    if processed_articles > 0:
        print(f"   â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: {total_time/processed_articles:.1f}ì´ˆ/ê¸°ì‚¬")
    else:
        print(f"   â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: ê³„ì‚° ë¶ˆê°€")
    if scraped_articles and save_to_db:
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘...")
        db = SessionLocal()
        try:
            save_result = save_articles_batch(db, scraped_articles)
            print(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²°ê³¼:")
            print(f"   â€¢ ì €ì¥ ì„±ê³µ: {save_result['saved']}ê°œ")
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        finally:
            db.close()
    elif scraped_articles and not save_to_db:
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê±´ë„ˆëœ€ (save_to_db=False)")
    return scraped_articles